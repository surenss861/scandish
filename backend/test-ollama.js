#!/usr/bin/env node

/**
 * Test script for Ollama integration
 * Run with: node test-ollama.js
 */

const axios = require('axios');

const OLLAMA_BASE_URL = process.env.OLLAMA_BASE_URL || 'http://localhost:11434';
const DEFAULT_MODEL = process.env.OLLAMA_DEFAULT_MODEL || 'llama3.1:8b';

async function testOllamaConnection() {
    console.log('ğŸ§  Testing Ollama Integration...\n');

    try {
        // Test 1: Health Check
        console.log('1ï¸âƒ£ Testing Ollama health...');
        const healthResponse = await axios.get(`${OLLAMA_BASE_URL}/api/tags`, {
            timeout: 5000
        });

        if (healthResponse.data && healthResponse.data.models) {
            console.log('âœ… Ollama is running!');
            console.log(`ğŸ“Š Available models: ${healthResponse.data.models.length}`);
            healthResponse.data.models.forEach(model => {
                console.log(`   - ${model.name} (${(model.size / 1024 / 1024 / 1024).toFixed(1)}GB)`);
            });
        } else {
            console.log('âŒ Ollama is running but no models found');
        }

        // Test 2: Model Availability
        console.log('\n2ï¸âƒ£ Checking default model...');
        const modelExists = healthResponse.data.models.some(model =>
            model.name === DEFAULT_MODEL || model.name.includes(DEFAULT_MODEL.split(':')[0])
        );

        if (modelExists) {
            console.log(`âœ… Default model ${DEFAULT_MODEL} is available`);
        } else {
            console.log(`âŒ Default model ${DEFAULT_MODEL} not found`);
            console.log('ğŸ’¡ Run: ollama pull llama3.1:8b');
        }

        // Test 3: Simple Generation
        console.log('\n3ï¸âƒ£ Testing AI generation...');
        const testPrompt = "Generate a simple Italian restaurant menu item with name, description, and price. Format as JSON.";

        const generateResponse = await axios.post(`${OLLAMA_BASE_URL}/api/generate`, {
            model: DEFAULT_MODEL,
            prompt: testPrompt,
            stream: false,
            options: {
                temperature: 0.7,
                num_predict: 200
            }
        }, {
            timeout: 30000
        });

        if (generateResponse.data && generateResponse.data.response) {
            console.log('âœ… AI generation working!');
            console.log('ğŸ“ Sample response:');
            console.log(generateResponse.data.response.substring(0, 200) + '...');
        } else {
            console.log('âŒ AI generation failed');
        }

        // Test 4: Menu Generation
        console.log('\n4ï¸âƒ£ Testing menu generation...');
        const menuPrompt = `You are a restaurant consultant. Generate a simple menu item for an Italian restaurant:

{
  "name": "Item Name",
  "description": "Description here",
  "price": 12.99,
  "category": "Appetizers",
  "emoji": "ğŸ½ï¸"
}

Make it authentic and appealing.`;

        const menuResponse = await axios.post(`${OLLAMA_BASE_URL}/api/generate`, {
            model: DEFAULT_MODEL,
            prompt: menuPrompt,
            stream: false,
            options: {
                temperature: 0.8,
                num_predict: 150
            }
        }, {
            timeout: 30000
        });

        if (menuResponse.data && menuResponse.data.response) {
            console.log('âœ… Menu generation working!');
            console.log('ğŸ½ï¸ Generated menu item:');
            console.log(menuResponse.data.response);
        } else {
            console.log('âŒ Menu generation failed');
        }

        console.log('\nğŸ‰ Ollama integration test completed successfully!');
        console.log('ğŸš€ Your Scandish AI Assistant is ready to use!');

    } catch (error) {
        console.error('\nâŒ Ollama integration test failed:');

        if (error.code === 'ECONNREFUSED') {
            console.error('ğŸ”Œ Connection refused - Ollama is not running');
            console.error('ğŸ’¡ Start Ollama with: ollama serve');
            console.error('ğŸ’¡ Or use Docker: docker run -d -p 11434:11434 ollama/ollama');
        } else if (error.code === 'ETIMEDOUT') {
            console.error('â° Request timed out - Model might be loading');
            console.error('ğŸ’¡ Try again in a few minutes');
        } else if (error.response) {
            console.error(`ğŸ“¡ HTTP ${error.response.status}: ${error.response.statusText}`);
            console.error(`ğŸ“„ Response: ${JSON.stringify(error.response.data)}`);
        } else {
            console.error(`ğŸš¨ ${error.message}`);
        }

        console.error('\nğŸ”§ Troubleshooting:');
        console.error('1. Ensure Ollama is running: ollama serve');
        console.error('2. Pull a model: ollama pull llama3.1:8b');
        console.error('3. Check port 11434 is available');
        console.error('4. Verify firewall settings');

        process.exit(1);
    }
}

// Run the test
if (require.main === module) {
    testOllamaConnection();
}

module.exports = { testOllamaConnection };

